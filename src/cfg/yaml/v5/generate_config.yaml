basic:
  seed: 42
  device: 'cuda'
  project_name: 'xlstm_fsdp'
  model_weight_path: "src/checkpoints/train-v4-cc100/model_0.pth"

dataset:
  name: 'ja_cc_wiki'
  debug: false
  min_seq_length: 15
  max_seq_length: 2048
  subset: ["train"]
  # subset: ["train", "validation", "test"]
  train_ratio: 0.8
  valid_ratio: 0.1
  test_ratio: 0.1
  pad_token_id: 50257

tokenizer:
  name: 'gpt2'

model:
  mlstm_block:
    mlstm:
      conv1d_kernel_size: 4
      qkv_proj_blocksize: 4
      num_heads: 4
      embedding_dim: 1024
  slstm_block:
    slstm:
      backend: cuda
      num_heads: 4
      conv1d_kernel_size: 4
      bias_init: powerlaw_blockdependent
      embedding_dim: 1024
    feedforward:
      proj_factor: 1.3
      act_fn: gelu
  context_length: ${dataset.max_seq_length}
  num_blocks: 48
  embedding_dim: 2048
  slstm_at: [3, 5, 7, 40, 42, 44]
  # NOTE: The following parameters is for the GPT2 model(https://huggingface.co/docs/transformers/en/model_doc/gpt2)
  # vocab size = gpt-2 vocab size + bos + eos + pad
  vocab_size: 50260

training:
  num_epochs: 3
  batch_size: 64
  lr: 0.0009
  val_every_step: 10000
  lr_warmup_steps: 750
  lr_decay_until_steps: ${.num_steps}
  lr_decay_factor: 0.001
  weight_decay: 0.1
  num_steps: 46585
  amp_precision: bfloat16
  weight_precision: float32
  enable_mixed_precision: true  
  model_save_dir: 'src/checkpoints/train-v5-cc100'
