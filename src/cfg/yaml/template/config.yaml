basic:
  seed: 42
  device: 'cuda'
  mode: 'train'
  project_name: 'xlstm_fsdp'
  project_tag: 'xlstm_test'
  model_weight_path: "${training.model_save_dir}/model_130000.pth"

dataset:
  name: 'ja_wiki' # slim_pajama | ja_wiki | ja_cc
  min_seq_length: 15
  max_seq_length: 2048
  subset: ["train"]
  # subset: ["train", "validation", "test"]

tokenizer:
  name: 'gpt2'

model:
  mlstm_block:
    mlstm:
      conv1d_kernel_size: 4
      qkv_proj_blocksize: 4
      num_heads: 2
      embedding_dim: 12
  slstm_block:
    slstm:
      backend: cuda
      num_heads: 2
      conv1d_kernel_size: 4
      bias_init: powerlaw_blockdependent
      embedding_dim: 12
    feedforward:
      proj_factor: 1.3
      act_fn: gelu
  context_length: ${dataset.max_seq_length}
  num_blocks: 4
  embedding_dim: 12
  slstm_at: [3]
  # NOTE: The following parameters is for the GPT2 model(https://huggingface.co/docs/transformers/en/model_doc/gpt2)
  # vocab size = gpt-2 vocab size + bos + eos + pad
  vocab_size: 50260

training:
  num_epochs: 3
  batch_size: 2
  grad_accum_steps: 8
  use_fsdp: true
  lr: 0.001
  val_every_step: 10
  val_steps: 10
  lr_warmup_steps: 750
  lr_decay_until_steps: ${.num_steps}
  lr_decay_factor: 0.001
  weight_decay: 0.1
  num_steps: 46585
  amp_precision: bfloat16
  weight_precision: float32
  enable_mixed_precision: true  
  model_save_dir: '/backup/tmp/mbaba/xlstm/checkpoints/train-${basic.project_tag}-${dataset.name}'
